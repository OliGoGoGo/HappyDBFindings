#!/usr/bin/env python
# coding: utf-8

# In[139]:


import io
import re
import csv
import bs4
import time
import string
import tqdm
import multiprocessing
import nltk
import spacy
import scipy.stats as stats
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as msno
import plotly.express as px
from textblob import TextBlob
from nltk.corpus import stopwords
from gensim.models import Word2Vec
from urllib import request
from wordcloud import WordCloud,STOPWORDS
from tqdm.notebook import trange, tqdm


# In[311]:


import scipy.stats as stats


# In[216]:


python -m spacy download en_core_web_sm


# In[18]:


import missingno as msno


# In[23]:


# 1: Preprocessing Data
folder = 'https://raw.githubusercontent.com/megagonlabs/HappyDB/master/happydb/data/'
cleaned_hm = pd.read_csv(folder+'cleaned_hm.csv', sep = ",",on_bad_lines='skip')
demographic = pd.read_csv(folder+'demographic.csv', sep = ",",on_bad_lines='skip')
senselable = pd.read_csv(folder+'senselabel.csv', sep = ',',on_bad_lines='skip')
topic = pd.read_csv(folder+'topic_dict', sep = ',',on_bad_lines='skip')


# In[30]:


senselable.head()


# In[6]:


cleaned_hm.head(10)


# In[19]:


## 1.1: Missing Value

msno.matrix(cleaned_hm)


# In[22]:


# from the missing plot, we can see that only ground_truth_category has many missing_values(color white)
cleaned_hm.drop('ground_truth_category', axis=1)


# In[ ]:


## 1.2: Tokenize
## 1.3: Lemmatization
# Because HappyDB provides senselabel.csv, which has already tokenized the sentences and lemmatized the words,
# I can simply connect the senselabel to cleaned_hm as a replacement of tokenization and lemmatization


# In[ ]:


## 1.4: StopWord Removing
## to reduce the redundant information, we did stopword removing to our texts


# In[71]:


# download the stopwords in English
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
noStopWords = senselable.copy()
# drop_condition = noStopWords['word'] in stop_words
noStopIndex = noStopWords.apply(lambda x: x['word'] not in stop_words, axis = 1)
noStopWords = senselable[noStopIndex]


# In[ ]:


## 1.5: punctuation removing


# In[273]:


veryCleanData = noStopWords[~noStopWords['lowercaseLemma'].str.contains(r'[!@#$%^&*()_+{}\[\]:;"<>,.?/~\\]').astype(bool)]


# In[279]:


reserve = len(veryCleanData)/len(senselable)
print('After Preprocess, we reserved {:.2f}% of the original texts'.format(reserve*100))


# In[ ]:


# 2: Explortory Data Analysis


# In[124]:


noStopWords = senselable[noStopIndex]


# In[140]:


# 2.1: word Clouds
def nuage_de_mots(text): 
    
    wordcloud = WordCloud(background_color="white",
                          relative_scaling = 1.0,
                          width=1280, 
                          height=800, 
                          stopwords = STOPWORDS # mots à ne pas prendre en compte dans le nuage
                          ).generate(text)
    plt.imshow(wordcloud)
    plt.axis("off")
    plt.show()  
    
def preprocessing_text(df):
    # On enlève les crochets et virgules et on les remplace par des tirets pour bien distinguer les mots-clés
    df.cleaned_hm = df.cleaned_hm.str.replace(r'[^\w\s]', '-')
    
    # Les seuls espaces possibles se trouvent au sein d'un même mot-clé ou bien entre deux mots-clés distincts
    df.cleaned_hm= df.cleaned_hm.str.replace(' ', '_')
    
    # On supprime les '-_-' qui ont été créés suite aux deux étapes précédentes
    df.cleaned_hm =  df.cleaned_hm.str.replace('-_-', '')

    # Désormais, les tirets sont la seule marque de ponctuation inutile
    df.cleaned_hm =  df.cleaned_hm.str.replace('-', ' ')
    return df 


def make_word_cloud(df):
    mots_cles = ''
    for i in trange(len(df)):
        mots_cles = mots_cles+str(df.cleaned_hm[i])

    nuage_de_mots(mots_cles)
    
def cleaning(doc):
    # Lemmatizes and removes stopwords
    # doc needs to be a spacy Doc object
    txt = [token.lemma_ for token in doc if not token.is_stop]
    # Word2Vec uses context words to learn the vector representation of a target word,
    # if a sentence is only one or two words long,
    # the benefit for the training is very small
    if len(txt) > 2:
        return ' '.join(txt)
    

def make_word_cloud_list(l):
    mots_cles = ''
    for i in trange(len(l)):
        mots_cles = mots_cles+str(l[i])

    nuage_de_mots(mots_cles)

make_word_cloud(cleaned_hm)

cleaned_hm

sns.histplot(cleaned_hm['num_sentence'])


# In[131]:


# 2.2: number of words for each happy moment
# we can see this as the excitement of a person, assuming that when people feels very happy, they tend to be more active
# and may write down more things
happyWords = senselable.groupby('hmid').count()
veryCleanDataWords = veryCleanData.groupby('hmid').count()


# In[133]:


fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))
ax1.hist(happyWords['word'], bins=1000, edgecolor='k', alpha=0.7) 
ax2.hist(veryCleanDataWords['word'], bins=1000, edgecolor='k', alpha=0.7) 
x_range = (0,175)
ax1.set_xlim(x_range)
ax1.set_xlabel('number of words in moments') 
ax1.set_ylabel('Frequency')  
ax1.set_title('Distribution of number of words')  
ax2.set_xlim(x_range)
ax2.set_xlabel('number of NoStopWords in moments')  
ax2.set_ylabel('Frequency')  
ax2.set_title('Distribution of number of NoStopWords')  

plt.show()


# In[187]:


print('we can see from the plot, after no stop process, the distribution becomes smoother, \n the distribution of the noStopWords skews to the right')


# In[ ]:


# 3: Question: how people express their happiness in various degrees


# In[ ]:


# Modelling

Sentiment analysis, Clustering techniques, Topic modelling (LDA, word2vec)...


# In[ ]:


# Thoughts: using Sentiment Analysis to find the distribution of happiness


# In[ ]:


nltk.download('punkt')  # download punctuation
# new sentiment column in the dataFrame
cleaned_hm['Sentiment'] = cleaned_hm['cleaned_hm'].apply(lambda x: TextBlob(x).sentiment.polarity)


# In[203]:


plt.hist(cleaned_hm['Sentiment'], bins=1000, edgecolor='k', alpha=0.7) 
cur_fig = plt.gcf()
xRange = (-0.25,1)
# 
plt.xlabel('Sentiment')
plt.ylabel('Frequency')
plt.title('Happiness Distribution')
plt.show()


# In[189]:


print('we can hardly get any information from the sentiment scores, \n0 score is too much, and the left distribution looks messy,\nstill want to save this analysis method, so I cut part of it to give a close look to scores distribution apart from 0')


# In[208]:


plt.hist(cleaned_hm['Sentiment'], bins=1000, edgecolor='k', alpha=0.7) 
cur_fig = plt.gcf()
xRange = (-0.25,1)
yRange = (0,1000)
plt.xlim(xRange)
plt.ylim(yRange)
plt.xlabel('Sentiment')
plt.ylabel('Frequency')
plt.title('Happiness Distribution')
plt.show()


# In[210]:


print('still hard to find anything,\nhas to change the method,\nconsidering word2Vector, because it can give more comprehensive score to each text')


# In[295]:


merged_data = cleaned_hm.merge(demographic, on='wid', how='inner')


# In[298]:


demographic


# In[301]:


demographic


# In[302]:


plt.figure(figsize=(10, 6))
sns.boxplot(data=merged_data, x='marital', y='Sentiment')
plt.title('Happiness Distribution by Marital Status')
plt.xlabel('Marital Status')
plt.ylabel('Happiness')
plt.xticks(rotation=45)


# In[329]:


male = merged_data['gender']=='m'
married = merged_data['marital']=='married'
single = merged_data['marital']=='single'
marriedMale = merged_data[male*married]['Sentiment']
marriedFemale = merged_data[(~male)*married]['Sentiment']
unmarMale = merged_data[male*single]['Sentiment']
unmarFemale = merged_data[(~male)*single]['Sentiment']


# In[332]:


f_statistic, p_value = stats.f_oneway(marriedMale, unmarMale, marriedFemale, unmarFemale)
print("F-statistic:", f_statistic)
print("p-value:", p_value)


# In[ ]:


# F_statistic very large and p_value < 0.05, gender and marital has significant influence on happiness


# In[388]:


pivot_table = merged_data.pivot_table(index='gender', columns='marital', values='Sentiment', aggfunc='mean')

plt.figure(figsize=(12, 8))
sns.heatmap(pivot_table, cmap='coolwarm', fmt='.2f', cbar=True)
plt.title('Happiness Heatmap by Gender, Marital')
plt.xlabel('')
plt.ylabel('')
plt.show()


# In[296]:


demographic.head()


# In[281]:


# produce the words DB for further use, transform the format into[[split_sentence1],[split_sentence2]...]
grouped = veryCleanData.groupby('hmid')['lowercaseLemma'].agg(list).reset_index()
trainData = grouped['lowercaseLemma'].tolist()


# In[285]:


# construct word2Vec model
model = gensim.models.Word2Vec(trainData, vector_size=100, window=5, min_count=1, sg=0)

# train the model
model.train(trainData, total_examples=len(trainData), epochs=10)


# In[432]:


model.wv.most_similar(positive=["coffee"])


# In[433]:


model.wv.most_similar(positive=["tea"])


# In[ ]:


# one interesting thing I found during the process, I found that the sentiment score for 'drink a coke' is 0
# so I want to check how many people report 'coke' related comments
# and further, I want to see the happy that food and drinks can bring to people
# my methods is to see the percentage of happiness gotten from food


# In[390]:





# In[420]:


water = sum(veryCleanData['lowercaseLemma'].apply(lambda x: x == 'water'))
alcohol = sum(veryCleanData['lowercaseLemma'].apply(lambda x: x == 'alcohol'))+sum(veryCleanData['lowercaseLemma'].apply(lambda x: x == 'beer'))+sum(veryCleanData['lowercaseLemma'].apply(lambda x: x == 'wine'))
tea = sum(veryCleanData['lowercaseLemma'].apply(lambda x: x == 'tea'))
coffee = sum(veryCleanData['lowercaseLemma'].apply(lambda x: x == 'starbucks'))+sum(veryCleanData['lowercaseLemma'].apply(lambda x: x == 'coffee'))+sum(veryCleanData['lowercaseLemma'].apply(lambda x: x == 'cappuccino'))
juice_smoothie = sum(veryCleanData['lowercaseLemma'].apply(lambda x: x == 'juice'))+sum(veryCleanData['lowercaseLemma'].apply(lambda x: x == 'smoothie'))


# In[429]:


coffee


# In[427]:


drinks = ['water','alcohol','tea','coffee','juice_smoothie']

# 喜好程度评分（假设分数在1到10之间，10表示最喜欢）
scores = [water,alcohol,tea,coffee,juice_smoothie]

# 创建柱状图
plt.figure(figsize=(8, 6))  # 设置图表尺寸
plt.bar(drinks, scores, color='skyblue')  # 绘制柱状图

# 添加标题和标签
plt.title('drinks popularity')
plt.xlabel('drinks')
plt.ylabel('popularity')

# 显示图表
plt.tight_layout()  # 调整布局，确保标签不重叠
plt.show()

